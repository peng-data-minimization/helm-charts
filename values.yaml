spi:
  streaming_platform:
    type: kafka
    #broker_url: {{ .Release.Name }}-cp-kafka-headless:9092
    broker_url: dm-pipeline-cp-kafka-headless:9092
    auth:
    - user: ~
    - pw: ~

  task_defaults:
    input_offset_reset: earliest
    topic_encoding: "utf8"
    storage_mode: "memory"
    input_data_type: "json"

  # tasks: []
  # - name: first
  #   input_topic: "foo"
  #   output_topic: "bar"
  #   function:
  #     name: "reduce_to_mean"
  #     args:
  #       keys: ['a']
  #     window:
  #       type: length
  #       value: 5
  #       grouping_keys: ['b']

elasticsearch:
  cluster:
    additionalJavaOpts: "-Xmx128m -Xms128m"

  # Old Elasticsearch version is required which is compatible with lensesio Kafka Elasticsearch connector
  image:
    tag: 6.8.2

  # Request smaller persistent volumes.
  data:
    persistence:
      storageClass: "standard"
      size: 5Gi

elasticsink:
  replicaCount: 1

  image:
    repository: streamreactor/elastic6
    tag: 1.2.7
    pullPolicy: IfNotPresent

  # Resource management
  resources:
    limits:
      memory: 512Mi
    requests:
      memory: 256Mi

  # Monitoring
  monitoring:
    pipeline: "test" # TODO to be configured
    enabled: true
    port: 9102
    path: "/metrics"

  podManagementPolicy: OrderedReady

  # kafka ssl
  # The key and truststores file data are the base64 encoded contents of the files. YOU MUST PROVIDE THE DATA BASE64 encoded
  # and added to the kafka secret and mounted into /mnt/connector-secrets
  kafka:
    # replicationFactor for connect topics
    replicationFactor: 2
    securityProtocol:
    ssl:
      enabled: false
      trustStoreFileData:
      trustStorePassword:
      keyStoreFileData:
      keyStorePassword:
    sasl:
      enabled: false
      # keyTabData is the contents kerberos keytab file is using kerberos
      keyTabData: |-

      # jaasFileData is the contents of the kafka jaas file
      jaasFileData: |-

      #GSSAPI, SCRAM or PLAIN
      mechanism: GSSAPI
      # kerberos krb5 contents
      krb5Conf: |-

    bootstrapServers:
      # - name: {{ .Release.Name }}-cp-kafka-headless
      - name: dm-pipeline-cp-kafka-headless
        port: 9092
        sslPort: 9093
        saslSslPort: 9094
        saslPlainTextPort: 9095

  schemaRegistries:
    enabled: true
    hosts:
      # - host: {{ .Release.Name }}-cp-schema-registry
      - host: dm-pipeline-cp-schema-registry
        protocol: http
        port: 8081
        jmxPort: 9102


  # secretsProvider is either env (k8), vault (hashicorp) or azure (keyvault)
  secretsProvider: env

  #javaHeap option
  javaHeap: "256M"

  # clusterName The connect cluster name. This is the consumer group id for the backing topics
  elasticClusterName: elasticsearch

  # restPort The rest port of Connect
  restPort: 8083

  # logLevel The log4j level
  logLevel: INFO

  # keyConverter The key converter to/from Connects struct
  keyConverter: org.apache.kafka.connect.storage.StringConverter

  # valueConverter The key converter to/from Connects struct
  valueConverter: org.apache.kafka.connect.storage.StringConverter

  # connectorClass
  connectorClass: com.datamountaineer.streamreactor.connect.elastic6.ElasticSinkConnector

  # applicationId name of the connector
  applicationId: com.datamountaineer.streamreactor.connect.elastic6.ElasticSinkConnector

  # topics to sink
  topics: "anon,processed"

  # kcqb KCQL expression describing field selection and routes. type: STRING importance: HIGH
  kcql: "INSERT INTO activities SELECT * FROM anon; INSERT INTO processed_activities SELECT * FROM processed;"


  # A dictionary allowing to specify SMT's, e.g.
  # transforms:
  #   transforms: "weeklysplit"
  #   transforms_weeklysplit_type: "org.apache.kafka.connect.transforms.TimestampRouter"
  #   transforms_weeklysplit_timestamp_format: "yyyy-ww"
  #   transforms_weeklysplit_topic_format: "${topic}-${timestamp}"
  transforms:

  # batchSize How many records to process at one time. As records are pulled from Kafka it can be 100k+ which will not be feasible to throw at Elastic search at once type: INT importance: MEDIUM
  batchSize: 4000

  # useHttp TCP or HTTP. Elastic4s client type to use, http or tcp, default is tcp. type: STRING importance: MEDIUM
  useHttp: tcp

  # clusterName Name of the elastic search cluster, used in local mode for setting the connection type: STRING importance: HIGH
  clusterName: elasticsearch

  # writeTimeout The time to wait in millis. Default is 5 minutes. type: INT importance: MEDIUM
  writeTimeout: 300000

  # xpackPlugins Provide the full class name for all the plugins you want to enable. type: STRING importance: MEDIUM
  # xpackPlugins:

  # urlPrefix URL connection string prefix type: STRING importance: LOW
  urlPrefix: elasticsearch

  # xpackSettings XpackSettings key in the secret. Enable xpack security add on by providing this setting type: PASSWORD importance: MEDIUM
  xpackSettings: |-
    "xpack.security.enabled=false"

  # url Url including port for Elastic Search cluster node. type: STRING importance: HIGH
  # url: {{ .Release.Name }}-elasticsearch-discovery:9300
  url: dm-pipeline-elasticsearch-discovery:9300

  # maxRetries The maximum number of times to try the write again. type: INT importance: MEDIUM
  maxRetries: 20

  # retryInterval The time in milliseconds between retries. type: INT importance: MEDIUM
  retryInterval: 60000

  # errorPolicy
  # Specifies the action to be taken if an error occurs while inserting the data.
  #  There are three available options:
  #     NOOP - the error is swallowed
  #     THROW - the error is allowed to propagate.
  #     RETRY - The exception causes the Connect framework to retry the message. The number of retries is set by connect.cassandra.max.retries.
  # All errors will be logged automatically, even if the code swallows them.
  #      type: STRING importance: HIGH
  errorPolicy: THROW

  # enabled Enables the output for how many records have been processed type: BOOLEAN importance: MEDIUM
  progressEnabled: true

kibana:
  env:
    # ELASTICSEARCH_HOSTS: http://{{ .Release.Name }}-elasticsearch-client:9200
    ELASTICSEARCH_HOSTS: http://dm-pipeline-elasticsearch-client:9200
  image:
    tag: 6.8.0

confluent-platform:
  ## ------------------------------------------------------
  ## Zookeeper
  ## ------------------------------------------------------
  cp-zookeeper:
    enabled: true
    servers: 3
    image: confluentinc/cp-zookeeper
    imageTag: 5.5.0
    ## Optionally specify an array of imagePullSecrets. Secrets must be manually created in the namespace.
    ## https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    imagePullSecrets:
    #  - name: "regcred"
    heapOptions: "-Xms512M -Xmx512M"
    persistence:
      enabled: true
      ## The size of the PersistentVolume to allocate to each Zookeeper Pod in the StatefulSet. For
      ## production servers this number should likely be much larger.
      ##
      ## Size for Data dir, where ZooKeeper will store the in-memory database snapshots.
      dataDirSize: 10Gi
      # dataDirStorageClass: ""

      ## Size for data log dir, which is a dedicated log device to be used, and helps avoid competition between logging and snaphots.
      dataLogDirSize: 10Gi
      # dataLogDirStorageClass: ""
    resources: {}
    ## If you do want to specify resources, uncomment the following lines, adjust them as necessary,
    ## and remove the curly braces after 'resources:'
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

  ## ------------------------------------------------------
  ## Kafka
  ## ------------------------------------------------------
  cp-kafka:
    enabled: true
    brokers: 3
    image: confluentinc/cp-enterprise-kafka
    imageTag: 5.5.0
    ## Optionally specify an array of imagePullSecrets. Secrets must be manually created in the namespace.
    ## https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    imagePullSecrets:
    #  - name: "regcred"
    heapOptions: "-Xms512M -Xmx512M"
    persistence:
      enabled: true
      # storageClass: ""
      size: 20Gi
      disksPerBroker: 1
    configurationOverrides:
      "offsets.topic.replication.factor": "3"
      "auto.create.topics.enable": true
      ## Options required for external access via NodePort
      ## ref:
      ## - http://kafka.apache.org/documentation/#security_configbroker
      ## - https://cwiki.apache.org/confluence/display/KAFKA/KIP-103%3A+Separation+of+Internal+and+External+traffic
      ##
      ## Advertised listeners will use the firstListenerPort value as it's default unless overridden here.
      ## Setting "advertised.listeners" here appends to "PLAINTEXT://${POD_IP}:9092,"
      "advertised.listeners": |-
        EXTERNAL://35.246.144.21:$((31090 + ${KAFKA_BROKER_ID}))
      "listener.security.protocol.map": |-
        PLAINTEXT:PLAINTEXT,EXTERNAL:PLAINTEXT
    nodeport:
      enabled: true
      servicePort: 19092
      firstListenerPort: 31090
    resources: {}
    ## If you do want to specify resources, uncomment the following lines, adjust them as necessary,
    ## and remove the curly braces after 'resources:'
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

  ## ------------------------------------------------------
  ## Schema Registry
  ## ------------------------------------------------------
  cp-schema-registry:
    enabled: true
    image: confluentinc/cp-schema-registry
    imageTag: 5.5.0
    ## Optionally specify an array of imagePullSecrets. Secrets must be manually created in the namespace.
    ## https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    imagePullSecrets:
    #  - name: "regcred"
    heapOptions: "-Xms512M -Xmx512M"
    resources: {}
    ## If you do want to specify resources, uncomment the following lines, adjust them as necessary,
    ## and remove the curly braces after 'resources:'
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi
    configurationOverrides:
      "cleanup.policy": "compact"


  ## ------------------------------------------------------
  ## REST Proxy
  ## ------------------------------------------------------
  cp-kafka-rest:
    enabled: true
    image: confluentinc/cp-kafka-rest
    imageTag: 5.5.0
    ## Optionally specify an array of imagePullSecrets. Secrets must be manually created in the namespace.
    ## https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    imagePullSecrets:
    #  - name: "regcred"
    heapOptions: "-Xms512M -Xmx512M"
    resources: {}
    ## If you do want to specify resources, uncomment the following lines, adjust them as necessary,
    ## and remove the curly braces after 'resources:'
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

  ## ------------------------------------------------------
  ## Kafka Connect
  ## ------------------------------------------------------
  cp-kafka-connect:
    enabled: true
    image: confluentinc/cp-kafka-connect
    imageTag: 5.5.0
    ## Optionally specify an array of imagePullSecrets. Secrets must be manually created in the namespace.
    ## https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    imagePullSecrets:
    #  - name: "regcred"
    heapOptions: "-Xms512M -Xmx512M"
    resources: {}
    ## If you do want to specify resources, uncomment the following lines, adjust them as necessary,
    ## and remove the curly braces after 'resources:'
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi

  ## ------------------------------------------------------
  ## KSQL Server
  ## ------------------------------------------------------
  cp-ksql-server:
    enabled: true
    image: confluentinc/cp-ksqldb-server
    imageTag: 5.5.0
    ## Optionally specify an array of imagePullSecrets. Secrets must be manually created in the namespace.
    ## https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    imagePullSecrets:
    #  - name: "regcred"
    heapOptions: "-Xms512M -Xmx512M"
    ksql:
      headless: false

  ## ------------------------------------------------------
  ## Control Center
  ## ------------------------------------------------------
  cp-control-center:
    enabled: true
    image: confluentinc/cp-enterprise-control-center
    imageTag: 5.5.0
    ## Optionally specify an array of imagePullSecrets. Secrets must be manually created in the namespace.
    ## https://kubernetes.io/docs/concepts/containers/images/#specifying-imagepullsecrets-on-a-pod
    imagePullSecrets:
    #  - name: "regcred"
    heapOptions: "-Xms512M -Xmx512M"
    resources: {}
    ## If you do want to specify resources, uncomment the following lines, adjust them as necessary,
    ## and remove the curly braces after 'resources:'
    #  limits:
    #   cpu: 100m
    #   memory: 128Mi
    #  requests:
    #   cpu: 100m
    #   memory: 128Mi
    configurationOverrides:
      "replication.factor": "3"
      "monitoring.interceptor.topic.replication": "3"
      "metrics.topic.replication": "3"
      "command.topic.replication": "3"
      "internal.topics.replication": "3"
      "internal.topics.partitions": "4"
